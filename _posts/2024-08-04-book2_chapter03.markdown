---
layout: post
title:  "8.04 Chapter 03"
date:   2024-08-04 21:14:31 +0900
categories: ML Session
toc: true
pin: true
image:
  path: thumbnail.png
  alt: image alternative text
---

# 회귀 알고리즘과 모델 규제

## k-최근접 이웃 회귀

* 지도 학습 알고리즘은 크게 **분류** 와 **회귀**로 나뉩니다.
* 회귀: 클래스 중 하나로 분류하는 것이 아니라 **임의의 어떤 숫자를 예측하는 문제**
* 이웃한 샘플의 타깃은 어떤 클래스가 아니라 임의의 수치입니다.

## 데이터 준비

## 결정계수(R^2)
* 회귀 모델에서 score 함수에 의한 평가값
* $R^2 = 1 - (타깃 - 예측)^2 / (타깃 - 평균)^2$

## 과대적합 vs 과소적합
* 훈련 세트에서 점수가 굉장히 좋았는데 테스트 세트에서는 점수가 굉장히 나빴다면 모델이 훈련 세트에 **과대적합(overfitting)** 되었다고 말합니다.
* 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 너무 낮은 경우 모델이 훈련 세트에 **과소적합(underfitting)** 되었다고 말합니다.

## 회귀 문제 다루기

## 확인 문제
1. 2

# 선형 회귀

## k-최근접 이웃의 한계

## 선형 회귀(linear regression)
* 선형이란 말에서 짐작할 수 있듯이 특성이 하나인 경우 어떤 직선을 학습하는 알고리즘입니다.
* *특성*과 *타깃* 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾습니다. 특성이 하나면 직선 방정식이 됩니다.

* coef_, intercept_ 를 머신러닝 알고리즘이 찾은 값이라는 의미로 **모델 파라미터(model parameter)**라고 부릅니다.
* 많은 머신러닝 알고리즘의 훈련과정은 최적의 모델 파라미터를 찾는 것과 같은데 이를 **모델 기반 학습**이라고 부릅니다.
* k-최근접 이웃 알고리즘처럼 모델 파라미터없이 훈련 세트를 저장하는 것이 훈련의 전부인 경우 **사례 기반 학습**이라고 부릅니다.

## 다항 회귀
* 다항식을 사용한 선형 회귀를 **다항 회귀**라고 부릅니다.

## 선형 회귀로 훈련 세트 범위 밖의 샘플 예측
* k-최근접 이웃 회귀를 사용했을 때의 문제는 **훈련 세트 범위 밖의 샘플을 예측할 수 없다는 점**입니다.
* k-최근접 이웃 회귀는 아무리 멀리 떨어져 있더라도 무조건 가장 가까운 샘플의 타깃을 평균하여 예측합니다.

## 확인 문제
1. 4
2. 1

# 특성 공학과 규제

## 다중 회귀
* 여러 개의 특성을 사용한 선형 회귀를 **다중 회귀**라고 부릅니다.
* 특성이 2개면 선형 회귀는 평면을 학습합니다.
* 선형 회귀를 단순한 직선이나 평면으로 생각하여 성능이 무조건 낮다고 오해해서는 안 됩니다. 특성이 많은 고차원에서는 선형 회귀가 매우 복잡한 모델을 표현할 수 있습니다.
* 기존의 특성을 사용해서 새로운 특성을 뽑아내는 작업을 **특성 공학**이라고 부릅니다.

## 데이터 준비
* **판다스**는 유명한 데이터 분석 라이브러리입니다.
* **데이터프레임**은 판다스의 핵심 데이터구조입니다.
* csv 파일은 다음 그림처럼 콤마로 나누어져 있는 텍스트 파일입니다.

## 사이킷런의 변환기
* 사이킷런은 특성을 만들거나 전처리하기 위한 다양한 클래스를 제공합니다. 사이킷런에서는 이런 클래스를 **변환기**라고 부릅니다.
* $fit()$ 메서드는 새롭게 만들 특성 조합을 찾고 $transform()$ 메서드는 실제로 데이터를 변환합니다.
* 변환기는 타깃 데이터 없이 입력 데이터를 변환합니다.

## 다중 회귀 모델 훈련하기
* 특성의 개수를 크게 늘리면 선형 모델은 아주 강력해지고, 훈련 세트에 대해 거의 완벽하게 학습할 수 있습니다.
* 하지만 이런 모델은 훈련 세트에 너무 과대적합되므로 테스트 세트에서는 형편없는 점수를 만듭니다.

## 규제
* 규제(regularization)는 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것을 말합니다.
* 선형 회귀 모델의 경우 특성에 곱해지는 계수(또는 기울기)의 크기를 작게 만드는 일입니다.
* **꼭 훈련 세트로 학습한 변환기를 사용해 테스트 세트까지 변환해야 합니다.**
* 선형 회귀 모델에 규제를 추가한 모델을 **릿지(ridge)**와 **라쏘(lasso)**라고 부릅니다.
* 릿지는 계수를 제곱한 값을 기준으로 규제를 적용하고, 라쏘는 계수의 절댓값을 기준으로 규제를 적용합니다.

## 릿지 회귀
* 머신러닝 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터를 **하이퍼파라미터(hyperparameter)**라고 부릅니다.

## 라쏘 회귀
* 릿지와 달리 계수 값을 아예 0으로 만들 수 있습니다. 이런 특징 때문에 라쏘 모델을 유용한 특성을 골라내는 용도로도 사용할 수 있습니다.

## 확인 문제
1. 4
2. 3
3. 2
