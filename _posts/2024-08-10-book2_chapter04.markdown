---
layout: post
title:  "8.10 Chapter 04"
date:   2024-08-04 21:14:31 +0900
categories: ML Session
---

# 로지스틱 회귀

## 럭키백의 확률
* 타깃 데이터에 2개 이상의 클래스가 포함된 문제를 **다중 분류(multi-class classification)**라고 부릅니다.

## 로지스틱 회귀
* 로지스틱 회귀(logistic regression)는 이름은 회귀이지만 분류 모델입니다. 이 알고리즘은 선형 회귀와 동일하게 선형 방정식을 학습합니다.
* 선형방정식의 결과는 어떤 값도 가능하지만 이 값이 *확률*이 되려면 **시그모이드 함수(sigmoid function) (또는 로지스틱 함수)**를 사용해야 합니다.
* 넘파이 배열은 True, False 값을 전달하여 행을 선택할 수 있는데, 이를 **불리언 인덱싱(boolean indexing)**이라고 합니다.

![epoch](https://github.com/thddlsgur0105/thddlsgur0105.github.io/blob/master/images/18.png)

### 로지스틱 회귀로 다중 분류 수행하기
* max_iter 매개변수에서 반복횟수를 지정하며 기본값은 100입니다.
* LogisticRegression은 기본적으로 릿지 회귀와 같이 계수의 제곱을 규제하고, 이를 L2 규제라고 합니다.
* 릿지 회귀에서의 규제의 양을 조절하는 alpha 매개변수처럼, C라는 매개변수를 사용해 규제를 제어합니다. 기본값은 1이고, 이 값은 작을수록 규제가 커집니다.
* 다중 분류는 클래스마다 z값을 하나씩 계산하고, 가장 높은 z값을 출력하는 클래스가 예측 클래스가 됩니다.
* 이진 분류에서 확률을 계산할 때 시그모이드 함수를 계산했듯, 다중 분류에서는 소프트맥스 함수를 사용해 7개의 z값을 확률로 변환합니다.

# 확률적 경사 하강법

## 점진적인 학습
* 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방법입니다.
* 대표적인 점진적 학습 알고리즘은 확률적 경사 하강법(stochastic gradient descent)입니다.

## 확률적 경사하강법
* 확률적이란 모델을 훈련하기 위해, 샘플을 사용할 때 훈련 세트에서 랜덤하게 고른다는 것을 의미합니다.
* 이처럼 훈련 세트에서 랜덤하게 하나의 샘플을 고르는 것을 확률적 경사 하강법이라고 합니다.
* 훈련 세트를 한 번 모두 사용하는 과정을 에포크(epoch)라고 부릅니다.
* 1개의 샘플 말고 무작위로 몇 개의 샘플을 사용해 경사 하강법을 수행하는 방식은 미니배치 경사 하강법(minibatch gradient descent)라고 부릅니다.

![epoch](https://github.com/thddlsgur0105/thddlsgur0105.github.io/blob/master/images/17.png)

## 손실함수(loss function)
* 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준입니다.
* 이 손실함수의 값은 작을수록 좋습니다.
* 대표적으로 로지스틱 손실 함수 혹은 이진 크로스엔트로피 손실 함수를 사용하고, 다중 분류에서는 크로스엔트로피 손실 함수를 사용합니다.

![epoch](https://github.com/thddlsgur0105/thddlsgur0105.github.io/blob/master/images/16.png)

## 에포크와 과대/과소적합
* 에포크가 너무 적으면 훈련 세트를 덜 학습하므로 과소적합이, 너무 많으면 과대적합이 될 수 있습니다.
* 따라서 훈련을 진행하면서 과대적합이 시작하기 전에 훈련을 멈추어 조기 종료를 해 주어야 합니다.

![epoch](https://github.com/thddlsgur0105/thddlsgur0105.github.io/blob/master/images/15.png)